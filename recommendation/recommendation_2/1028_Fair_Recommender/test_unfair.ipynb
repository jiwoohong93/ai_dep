{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ot\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import time\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2000\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NumpyDataset:\n",
    "    def __init__(self, data, colors):\n",
    "        self.data = data\n",
    "        self.colors = colors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.colors[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class NumpyDataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True, drop_last=False, output_ids=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.output_ids = output_ids\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.indices = np.arange(len(self.dataset))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        if self.drop_last:\n",
    "            self.indices = self.indices[:len(self.indices) - len(self.indices) % self.batch_size]\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if len(self.indices) == 0:\n",
    "            raise StopIteration\n",
    "        \n",
    "        indices = self.indices[:self.batch_size]\n",
    "        self.indices = self.indices[self.batch_size:]\n",
    "        \n",
    "        if self.output_ids:\n",
    "            batch = []\n",
    "            for i in indices:\n",
    "                batch.append((i, self.dataset[i][0], self.dataset[i][1]))\n",
    "            ids, data, labels = zip(*batch)\n",
    "            return np.array(ids), np.array(data), np.array(labels)\n",
    "        else:\n",
    "            batch = [self.dataset[i] for i in indices]\n",
    "            data, labels = zip(*batch)\n",
    "            return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "def optimize_coupling(xs, centers, numItermax=5000, numThreads=3):\n",
    "\n",
    "    # only for binary sensitive attribute\n",
    "    n_colors = 2\n",
    "    assert n_colors == len(xs)\n",
    "    ns = [len(xs_i) for xs_i in xs]\n",
    "    pis = np.array(ns) / np.sum(ns)\n",
    "    # pi_0, pi_1 =  pis[0], pis[1]\n",
    "    pi_0, pi_1 = 0.99, 0.01\n",
    "\n",
    "    n_0, n_1 = ns[0], ns[1]\n",
    "    w_0, w_1 = np.ones(n_0) / n_0, np.ones(n_1) / n_1\n",
    "    \n",
    "    D_01 = 2 * pi_0 * pi_1 * ot.dist(xs[0], xs[1])\n",
    "    \n",
    "    Taxs_0_1 = pi_0 * np.repeat(xs[0], n_1, 0) + pi_1 * np.tile(xs[1], (n_0, 1))\n",
    "    mid_distances = distance.cdist(Taxs_0_1, centers, metric='minkowski', p=2)\n",
    "    mid_assignments = np.argmin(mid_distances, axis=1)\n",
    "    C_01 = (mid_distances[np.arange(mid_distances.shape[0]), mid_assignments]**2)\n",
    "    # C_mami = C_mami.reshape(n_1, n_0).T\n",
    "    C_01 = C_01.reshape(n_0, n_1)\n",
    "    \n",
    "    Coupling_0_1 = ot.emd(w_0, w_1, D_01+C_01, numItermax=numItermax, numThreads=numThreads)\n",
    "\n",
    "    return Taxs_0_1, Coupling_0_1.flatten(), n_0, n_1\n",
    "\n",
    "\n",
    "\n",
    "def optimize_center(Taxs, gammas, centers, centers_module, centers_optimizer, K, seed=2024, gradient_descent=False, use_cuda=False):\n",
    "    if gradient_descent:\n",
    "        Taxs = torch.from_numpy(Taxs).float().cuda() if use_cuda else torch.from_numpy(Taxs).float()\n",
    "        gammas = torch.from_numpy(gammas).float().cuda() if use_cuda else torch.from_numpy(gammas).float()\n",
    "        for _ in range(20):\n",
    "            distances = torch.cdist(Taxs, centers_module.weight, p=2)\n",
    "            assignments = torch.argmin(distances, dim=1)\n",
    "            energy = (gammas * (distances[torch.arange(distances.shape[0]), assignments]**2)).sum()\n",
    "            centers_optimizer.zero_grad()\n",
    "            energy.backward()\n",
    "            centers_optimizer.step()\n",
    "        new_centers = centers_module.weight.data.cpu().detach().numpy()\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=K, init=centers, random_state=seed)\n",
    "        # kmeans = KMeans(n_clusters=K, random_state=seed)\n",
    "        kmeans.fit(X=Taxs, sample_weight=gammas)\n",
    "        new_centers = kmeans.cluster_centers_\n",
    "    return new_centers\n",
    "\n",
    "\n",
    "\n",
    "def random_hard_assigning(arr):\n",
    "    max_values = np.max(arr, axis=1)\n",
    "    max_mask = arr == max_values[:, None]\n",
    "    chosen_indices = np.array([np.random.choice(np.where(row)[0]) for row in max_mask])\n",
    "    return chosen_indices\n",
    "\n",
    "\n",
    "\n",
    "def assigning(xs, Taxs, gammas, centers, K):\n",
    "    \"\"\"\n",
    "    xs: (n, d) # n = n0 + n1\n",
    "    colors: (n, )\n",
    "    Taxs: (n0 * n1, d)\n",
    "    gammas: (n0 * n1, )\n",
    "    n_majors: (B, ) # B = batch_size\n",
    "    n_minors: (B, ) # B = batch_size\n",
    "    centers: (K, d)\n",
    "    K: int\n",
    "    \"\"\"\n",
    "    \n",
    "    color_xs = [[], []]\n",
    "    color_assignments = [[], []]\n",
    "    for sub_xs, sub_Taxs, sub_gammas in zip(xs, Taxs, gammas):\n",
    "        \n",
    "        n_0, n_1 = sub_xs[0].shape[0], sub_xs[1].shape[0]\n",
    "        \n",
    "        sub_distances = distance.cdist(sub_Taxs, centers, metric='minkowski', p=2)\n",
    "        sub_assignments = np.argmin(sub_distances, axis=1)\n",
    "        \n",
    "        # shape: (n0, n1)\n",
    "        sub_gammas_i_j = sub_gammas.reshape(n_0, n_1)\n",
    "        sub_assignments_i_j = sub_assignments.reshape(n_0, n_1)\n",
    "\n",
    "        # for s = 0\n",
    "        prob_assignments_0 = np.zeros(shape=(n_0, K))\n",
    "        for row in range(n_0):\n",
    "            for k in range(K):\n",
    "                prob_assignments_0[row, k] = n_0 * np.sum(sub_gammas_i_j[row][sub_assignments_i_j[row] == k])\n",
    "\n",
    "        # for s = 1\n",
    "        prob_assignments_1 = np.zeros(shape=(n_1, K))\n",
    "        for col in range(n_1):\n",
    "            for k in range(K):\n",
    "                prob_assignments_1[col, k] = n_1 * np.sum(sub_gammas_i_j.T[col][sub_assignments_i_j.T[col] == k])\n",
    "\n",
    "        color_xs[0].append(sub_xs[0])\n",
    "        color_xs[1].append(sub_xs[1])\n",
    "        color_assignments[0].append(random_hard_assigning(prob_assignments_0))\n",
    "        color_assignments[1].append(random_hard_assigning(prob_assignments_1))\n",
    "    \n",
    "    color_xs[0] = np.concatenate(color_xs[0])\n",
    "    color_xs[1] = np.concatenate(color_xs[1])\n",
    "    color_assignments[0] = np.concatenate(color_assignments[0])\n",
    "    color_assignments[1] = np.concatenate(color_assignments[1])\n",
    "\n",
    "    return color_xs, color_assignments\n",
    "\n",
    "\n",
    "\n",
    "def evaluation(color_xs, color_assignments, centers, n_color, K):\n",
    "    \n",
    "    # objectives\n",
    "    cluster_cnts = []\n",
    "    objective = 0.0\n",
    "    for xs_i, assignments_i in zip(color_xs, color_assignments):\n",
    "        distances_i = distance.cdist(xs_i, centers, metric='minkowski', p=2)\n",
    "        objective += (distances_i[np.arange(distances_i.shape[0]), assignments_i]**2).sum()\n",
    "        \n",
    "        sub_cluster_cnts = []\n",
    "        for k in range(K):\n",
    "            sub_cluster_cnts.append((assignments_i == k).sum())\n",
    "        cluster_cnts.append(sub_cluster_cnts)\n",
    "    \n",
    "    # balance\n",
    "    all_cluster_cnts = np.array(cluster_cnts).sum(axis=0)\n",
    "    k_ratio = all_cluster_cnts / all_cluster_cnts.sum()\n",
    "    s_balances = []\n",
    "    for color in range(n_color):\n",
    "        s_balances.append((np.array(cluster_cnts[color]) / np.sum(cluster_cnts[color])) / k_ratio)\n",
    "    balance = np.array(s_balances).min(axis=1).min()\n",
    "\n",
    "    return objective, balance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed= 2024\n",
    "full_batch=True\n",
    "batch_size= 4096\n",
    "\n",
    "gradient_descent = True\n",
    "use_cuda = True\n",
    "\n",
    "iters = 10\n",
    "numItermax = 1000\n",
    "\n",
    "l2_normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path =\"data_combined_new.csv\"\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.iloc[:,7:].fillna(0)\n",
    "# new_data = new_data.values / new_data.values.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "# new_data_gender = [0]*len(data)\n",
    "new_data_gender = data.sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    np_data = np.array(new_data)\n",
    "    np_colors = np.array(new_data_gender)\n",
    "    K = 5\n",
    "    d = np_data.shape[1]\n",
    "    n_color = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Data shape for 0th color: (4331, 36)\n",
      "[Info] Data shape for 1th color: (1709, 36)\n"
     ]
    }
   ],
   "source": [
    "    xs = []\n",
    "    for color in range(n_color):\n",
    "        xs.append(np_data[np_colors == color])\n",
    "        print(f'[Info] Data shape for {color}th color: {np_data[np_colors == color].shape}')\n",
    "    colors = [i*np.ones(xs_i.shape[0]) for i, xs_i in enumerate(xs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    dset = NumpyDataset(np.concatenate(xs), np.concatenate(colors))\n",
    "    batch_size = batch_size\n",
    "    if full_batch:\n",
    "        batch_size = len(np.concatenate(xs))\n",
    "    dloader = NumpyDataLoader(dset, batch_size=batch_size,\n",
    "                              shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Unfair] Energy / Balance: 31468348.052 / 0.342\n"
     ]
    }
   ],
   "source": [
    "    # unfair clustering!\n",
    "    kmeans = KMeans(n_clusters=K, random_state=2024)\n",
    "    kmeans.fit(X=np.concatenate(xs))\n",
    "    unfair_assignments = kmeans.predict(X=np.concatenate(xs))\n",
    "    unfair_assignments = [unfair_assignments[:xs[0].shape[0]], unfair_assignments[xs[0].shape[0]:]]\n",
    "    unfair_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    objective, balance = evaluation(xs, unfair_assignments, unfair_centers, n_color, K)\n",
    "    print(f'[Unfair] Energy / Balance: {objective:.3f} / {balance:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # initial centers\n",
    "    centers = np.random.normal(0, 1, (K, d))\n",
    "    centers_module = None\n",
    "    centers_optimizer = None\n",
    "    if gradient_descent:\n",
    "        lr = 5e-3 \n",
    "        centers_module = nn.Linear(d, K, bias=False)\n",
    "        centers_optimizer = torch.optim.Adam(centers_module.parameters(), lr=lr)\n",
    "        if use_cuda:\n",
    "            centers_module = centers_module.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # optimizing\n",
    "    elapsed_times = []\n",
    "    best_it, best_original_energy, best_energy, best_balance = 0, 1e+10, 1e+10, 0.0\n",
    "    subbest_it, subbest_original_energy, subbest_energy, subbest_balance = 0, 1e+10, 1e+10, 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/nine/lib/python3.9/site-packages/ot/lp/__init__.py:354: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Energy / Balance: 194767340.683 / 0.812\n",
      "[2/10] Energy / Balance: 194426670.001 / 0.765\n",
      "[3/10] Energy / Balance: 194125800.546 / 0.815\n",
      "[4/10] Energy / Balance: 193819699.361 / 0.797\n",
      "[5/10] Energy / Balance: 193538081.526 / 0.730\n",
      "[6/10] Energy / Balance: 193248024.768 / 0.765\n",
      "[7/10] Energy / Balance: 192998963.297 / 0.751\n",
      "[8/10] Energy / Balance: 192702360.102 / 0.829\n",
      "[9/10] Energy / Balance: 192498435.736 / 0.793\n",
      "[10/10] Energy / Balance: 192158444.982 / 0.787\n",
      "[BEST balance/10] Energy / Balance: 192702360.102 / 0.829\n",
      "[BEST energy/10] Energy / Balance: 192158444.982 / 0.787\n"
     ]
    }
   ],
   "source": [
    "    for it in range(iters):\n",
    "        start_time = time.time()\n",
    "        it += 1\n",
    "        if gradient_descent:\n",
    "            centers = centers_module.weight.data.cpu().detach().numpy()\n",
    "        \n",
    "        all_xs, all_Taxs, all_gammas, all_colors = [], [], [], []\n",
    "        all_n_0, all_n_1 = [], []\n",
    "        for batch_xs, batch_colors in dloader:\n",
    "            sub_batch_xs = []\n",
    "            for color in range(n_color):\n",
    "                sub_batch_xs.append(batch_xs[batch_colors == color])\n",
    "            sub_batch_colors = [i*np.ones(sub_batch_xs_i.shape[0]) for i, sub_batch_xs_i in enumerate(sub_batch_xs)]\n",
    "            sub_Tax_i_j, sub_gamma_i_j, sub_n_0, sub_n_1 = optimize_coupling(sub_batch_xs, centers, numItermax=numItermax)\n",
    "            \n",
    "            all_xs.append(sub_batch_xs)\n",
    "            all_Taxs.append(sub_Tax_i_j)\n",
    "            all_gammas.append(sub_gamma_i_j)\n",
    "            all_colors.append(np.concatenate(sub_batch_colors))\n",
    "            all_n_0.append(sub_n_0)\n",
    "            all_n_1.append(sub_n_1)\n",
    "\n",
    "        all_colors = np.concatenate(all_colors)\n",
    "\n",
    "        # finding center\n",
    "        flat_all_Taxs = np.concatenate(all_Taxs)\n",
    "        flat_all_gammas = np.concatenate([sub_gamma / sub_gamma.shape[0] for sub_gamma in all_gammas])\n",
    "        centers = optimize_center(flat_all_Taxs, flat_all_gammas,\n",
    "                                  centers, centers_module, centers_optimizer, K, seed=seed,\n",
    "                                  gradient_descent=gradient_descent, use_cuda=use_cuda)\n",
    "\n",
    "        elapsed_times.append(time.time() - start_time)\n",
    "\n",
    "        color_xs, color_assignments = assigning(all_xs, all_Taxs, all_gammas, centers, K)\n",
    "        objective, balance = evaluation(color_xs, color_assignments, centers, n_color, K)\n",
    "\n",
    "        print(f'[{it}/{iters}] Energy / Balance: {objective:.3f} / {balance:.3f}')\n",
    "        \n",
    "        if balance > best_balance:\n",
    "            best_it = it\n",
    "            best_balance = balance\n",
    "            best_energy = objective\n",
    "            best_original_energy = objective\n",
    "        if objective < subbest_energy:\n",
    "            subbest_it = it\n",
    "            subbest_balance = balance\n",
    "            subbest_energy = objective\n",
    "            subbest_original_energy = objective\n",
    "            \n",
    "    elapsed_time_per_iter = np.mean(elapsed_times)\n",
    "    elapsed_time_total = np.sum(elapsed_times)\n",
    "\n",
    "    # results\n",
    "    results = {'type': [type],\n",
    "                'seed':[seed],\n",
    "                'gradient_descent': [gradient_descent],\n",
    "                'iters': [iters],\n",
    "                'full_batch': [f'real_{full_batch}'],\n",
    "                'batch_size': [batch_size],\n",
    "                'l2_normalize': [l2_normalize],\n",
    "                'elapsed_time_per_iter': [elapsed_time_per_iter],\n",
    "                'elapsed_time_total': [elapsed_time_total],\n",
    "                'best_it': [best_it],\n",
    "                'best_original_energy': [best_original_energy],\n",
    "                'best_energy': [best_energy],\n",
    "                'best_balance': [best_balance],\n",
    "                'subbest_it': [subbest_it],\n",
    "                'subbest_original_energy': [subbest_original_energy],\n",
    "                'subbest_energy': [subbest_energy],\n",
    "                'subbest_balance': [subbest_balance]\n",
    "                }\n",
    "\n",
    "    columns = list(results.keys())\n",
    "    df_results = pd.DataFrame(results, columns=columns)\n",
    "    #result_name = f'results/FCA/{args.data_name}_FCA.csv'\n",
    "\n",
    "    print(f'[BEST balance/{iters}] Energy / Balance: {best_energy:.3f} / {best_balance:.3f}')\n",
    "    print(f'[BEST energy/{iters}] Energy / Balance: {subbest_energy:.3f} / {subbest_balance:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811092715231788\n"
     ]
    }
   ],
   "source": [
    "# Grouping\n",
    "score = 0.0\n",
    "num_data = 0.0\n",
    "for group_num in [0,1,2,3,4]:\n",
    "\n",
    "    group_man_bool = color_assignments[0] == group_num\n",
    "    group_woman_bool = color_assignments[1] == group_num\n",
    "    group_man = color_xs[0][group_man_bool,:]\n",
    "    group_woman = color_xs[1][group_woman_bool,:]\n",
    "\n",
    "    group_all = np.concatenate([group_man,group_woman])\n",
    "\n",
    "    recommend_genre = pd.DataFrame( group_all[:,18:].argmax(axis=1) ).value_counts()\n",
    "\n",
    "    recommend_genre_0 = recommend_genre.index[0][0]\n",
    "    recommend_genre_1 = recommend_genre.index[1][0]\n",
    "    recommend_genre_2 = recommend_genre.index[2][0]\n",
    "\n",
    "    #score = 0.0\n",
    "    score += np.sum( (group_all[:,18:][:,recommend_genre_0] > 3.0) | (group_all[:,18:][:,recommend_genre_1] > 3.0) )\n",
    "    num_data += group_all.shape[0]\n",
    "print(score/num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man :0.8450704225352113 , Woman : 0.860655737704918\n",
      "Man :0.854320987654321 , Woman : 0.8108108108108109\n",
      "Man :0.6163069544364509 , Woman : 0.581081081081081\n",
      "Man :0.7412587412587412 , Woman : 0.7954545454545454\n",
      "Man :0.8639788997739262 , Woman : 0.7842968075927523\n"
     ]
    }
   ],
   "source": [
    "## 성별 별로\n",
    "for group_num in [0,1,2,3,4]:\n",
    "    group_man_bool = color_assignments[0] == group_num\n",
    "    group_woman_bool = color_assignments[1] == group_num\n",
    "    group_man = color_xs[0][group_man_bool,:]\n",
    "    group_woman = color_xs[1][group_woman_bool,:]\n",
    "\n",
    "    group_all = np.concatenate([group_man,group_woman])\n",
    "\n",
    "    recommend_genre = pd.DataFrame( group_all[:,18:].argmax(axis=1) ).value_counts()\n",
    "\n",
    "    recommend_genre_0 = recommend_genre.index[0][0]\n",
    "    recommend_genre_1 = recommend_genre.index[1][0]\n",
    "    recommend_genre_2 = recommend_genre.index[2][0]\n",
    "    \n",
    "    rating_posivie = 3.0\n",
    "    \n",
    "    man_score = 0.0\n",
    "    man_score += np.sum( (group_man[:,18:][:,recommend_genre_0] > rating_posivie) | (group_man[:,18:][:,recommend_genre_1] > rating_posivie) )\n",
    "    \n",
    "    \n",
    "    woman_score = 0.0\n",
    "    woman_score += np.sum( (group_woman[:,18:][:,recommend_genre_0] > rating_posivie) | (group_woman[:,18:][:,recommend_genre_1] > rating_posivie) )\n",
    "    \n",
    "    print(f\"Man :{man_score/group_man.shape[0]} , Woman : {woman_score/group_woman.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man :0.9194181482336643 , Woman : 0.9040374488004681 , Diff : 0.015380699433196199 \n"
     ]
    }
   ],
   "source": [
    "## 성별 별로\n",
    "man_score = 0.0\n",
    "woman_score = 0.0\n",
    "man_num = 0.0\n",
    "woman_num =0.0\n",
    "\n",
    "for group_num in [0,1,2,3,4]:\n",
    "    group_man_bool = color_assignments[0] == group_num\n",
    "    group_woman_bool = color_assignments[1] == group_num\n",
    "    group_man = color_xs[0][group_man_bool,:]\n",
    "    group_woman = color_xs[1][group_woman_bool,:]\n",
    "\n",
    "    group_all = np.concatenate([group_man,group_woman])\n",
    "\n",
    "    recommend_genre = pd.DataFrame( group_all[:,18:].argmax(axis=1) ).value_counts()\n",
    "\n",
    "    recommend_genre_0 = recommend_genre.index[0][0]\n",
    "    recommend_genre_1 = recommend_genre.index[1][0]\n",
    "    recommend_genre_2 = recommend_genre.index[2][0]\n",
    "    \n",
    "    rating_posivie = 3.00\n",
    "\n",
    "    man_score += np.sum( (group_man[:,18:][:,recommend_genre_0] >= rating_posivie) | (group_man[:,18:][:,recommend_genre_1] >= rating_posivie) )\n",
    "    \n",
    "    woman_score += np.sum( (group_woman[:,18:][:,recommend_genre_0] >= rating_posivie) | (group_woman[:,18:][:,recommend_genre_1] >= rating_posivie) )\n",
    "    \n",
    "    man_num += group_man.shape[0]\n",
    "    woman_num += group_woman.shape[0]\n",
    "    \n",
    "print(f\"Man :{man_score/man_num} , Woman : {woman_score/woman_num} , Diff : {np.abs( man_score/man_num- woman_score/woman_num ) } \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
