{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# path 변수.\n",
    "GBSQA_PATH = './dataset/GBSQA/GBS_QA_FF.csv' \n",
    "GBSQA_HOME_PATH = './dataset/GBSQA'\n",
    "HF_CACHE_PATH = './HuggingFaceCache/models' # huggingface에서 받는 모델이 저장되는 곳\n",
    "DATA_CACHE_PATH = '/HuggingFaceCache/dataset' # huggingface에서 받는 데이터셋이 저장되는 곳\n",
    "os.environ['TRANSFORMERS_CACHE'] = HF_CACHE_PATH # huggingface 캐쉬 디렉토리 지정. 모델이 다운 받는 위치 지정.\n",
    "os.environ['HF_DATASETS_CACHE'] = DATA_CACHE_PATH # 데이터셋 다운 받는 위치 지정\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' # PCI BUS ID로 현재 보이는 그래픽카드 정렬\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2' # 내가 할당된 그래픽카드만 시스템에서 보이도록 설정.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, AutoModel\n",
    "torch.cuda.current_device() # 0으로 출력되지만, 실제로 모델을 넣어 구동하면 지정한 그래픽카드에 할당될 것임.\n",
    "import pickle\n",
    "import evaluate\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from aim.hugging_face import AimCallback\n",
    "from aim import Run\n",
    "\n",
    "# For reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(li): # dataset 구분\n",
    "    num_test = 20 # 최소한의 test set 갯수 확보\n",
    "    train = li[:int(len(li) * 0.7)]\n",
    "    valid = li[int(len(li) * 0.7): len(li) - num_test]\n",
    "    test = li[len(li) - num_test:]\n",
    "    return train, valid, test\n",
    "\n",
    "def save_shuffle_indices(shuffled_pair, original_pair):\n",
    "    _o_pair = np.array(original_pair)\n",
    "    _s_pair = np.array(shuffled_pair)\n",
    "    find_idx_func = lambda x: np.where((_o_pair[:, 0] == x[0]) & (_o_pair[:, 1] == x[1]))\n",
    "    return np.array(list(map(find_idx_func, _s_pair))).squeeze()\n",
    "    \n",
    "def shuffle_array(array):\n",
    "    np.random.shuffle(array)\n",
    "    return array\n",
    "\n",
    "##\n",
    "# get ready dataset\n",
    "def load_preprocess_data(fpath: os.PathLike = './dataset/GBSQA/provision_intersected.csv'): \n",
    "\n",
    "    gbsqa = pd.read_csv(GBSQA_HOME_PATH + '/provision_intersected.csv')\n",
    "    valid_gbsqa = gbsqa[~gbsqa['provision'].isna()] # 전문가의 자세한 답변이 담긴 사항이 없는 것들 제외.\n",
    "    valid_gbsqa = valid_gbsqa[~valid_gbsqa['Classification'].isna()] # classification이 있는 것만 추출.\n",
    "    valid_gbsqa.drop_duplicates(subset=['Original answers', 'provision_text'], inplace=True)\n",
    "\n",
    "    text_pair = valid_gbsqa[['Original answers', 'provision_text']]\n",
    "    text_pair = list(text_pair.itertuples(index=False, name=None))\n",
    "    shuffled_pair = shuffle_array(text_pair.copy())\n",
    "    shuffle_indices = save_shuffle_indices(shuffled_pair, text_pair)\n",
    "\n",
    "    text_pair = valid_gbsqa[['Original answers', 'provision_text']]\n",
    "    train_indices, valid_indices, test_indices = dataset_split(shuffle_indices)\n",
    "    raw_dataset = {'train': text_pair.iloc[train_indices], 'valid': text_pair.iloc[valid_indices], 'test': text_pair.iloc[test_indices]}\n",
    "    \n",
    "    return raw_dataset, valid_gbsqa, shuffle_indices\n",
    "\n",
    "raw_dataset, valid_gbsqa, shuffle_indices = load_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_tokenizer = AutoTokenizer.from_pretrained(HF_CACHE_PATH + '/xlm_roberta_gbsqa_classification/tokenizer')\n",
    "\n",
    "labels = valid_gbsqa['Classification']\n",
    "zero_one = labels.map({'Yes':1, 'No':0})\n",
    "\n",
    "valid_gbsqa = valid_gbsqa[~zero_one.isna()]\n",
    "valid_gbsqa['Classification'] = zero_one\n",
    "\n",
    "def shuffle_array(array):\n",
    "    np.random.shuffle(array)\n",
    "    return array\n",
    "\n",
    "def tokenize_preprocess_classification(examples):\n",
    "    return class_tokenizer(*examples,max_length=512, padding='max_length', truncation=True)\n",
    "\n",
    "def dataset_split(li):\n",
    "    num_test = 20\n",
    "    train = li[:int(len(li) * 0.7)]\n",
    "    valid = li[int(len(li) * 0.7): len(li) - num_test]\n",
    "    test = li[len(li) - num_test:]\n",
    "    return train, valid, test\n",
    "\n",
    "def save_shuffle_indices(shuffled_pair, original_pair):\n",
    "    _o_pair = np.array(original_pair)\n",
    "    _s_pair = np.array(shuffled_pair)\n",
    "    find_idx_func = lambda x: np.where((_o_pair[:, 0] == x[0]) & (_o_pair[:, 1] == x[1]))\n",
    "    return np.array(list(map(find_idx_func, _s_pair))).squeeze()\n",
    "\n",
    "\n",
    "text_pair = valid_gbsqa[['Original answers', 'Revised questions', 'Classification']] \n",
    "text_pair = text_pair.iloc[shuffle_indices]\n",
    "text_pair = list(text_pair.itertuples(index=False, name=None))\n",
    "tokenized_classification = {}\n",
    "tp_np = np.array(text_pair)\n",
    "\n",
    "raw_dataset_classification_train, raw_dataset_classification_valid, raw_dataset_classification_test = dataset_split(tp_np.tolist())\n",
    "\n",
    "class GBSQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        self.length = len(encodings)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "tokenized_classification['train'] = GBSQADataset([dict({'labels': int(float(label))}, **tokenize_preprocess_classification([text_a, text_b])) for text_a, text_b, label in raw_dataset_classification_train])\n",
    "tokenized_classification['valid'] = GBSQADataset([dict({'labels': int(float(label))}, **tokenize_preprocess_classification([text_a, text_b]))for text_a, text_b, label in raw_dataset_classification_valid])\n",
    "tokenized_classification['test'] = GBSQADataset([dict({'labels': int(float(label))}, **tokenize_preprocess_classification([text_a, text_b])) for text_a, text_b, label in raw_dataset_classification_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pretraining on revised datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing 셀 블록을 실행한 이후에 사용 가능.\n",
    "model_repository = {'roberta':'roberta-base', \"deberta\":\"microsoft/deberta-v3-base\", \"xlm-roberta\":\"xlm-roberta-base\", \"xlm-large\":\"xlm-roberta-large\"}\n",
    "model_selection = \"xlm-large\"\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(HF_CACHE_PATH + '/xlm_roberta_gbsqa_classification/tokenizer')\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_repository[model_selection]) # get ready from pre-trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_preprocess(examples):\n",
    "    return new_tokenizer(*examples, max_length=512,truncation=True) # 위에 정의한 new_tokenizer.\n",
    "train = list(raw_dataset['train'].to_numpy())\n",
    "valid = list(raw_dataset['valid'].to_numpy())\n",
    "tokenized_train = list(map(tokenize_preprocess,train))\n",
    "tokenized_valid = list(map(tokenize_preprocess, valid))\n",
    "tokenized_train.extend(tokenized_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_tokenizer.pad_token = new_tokenizer.eos_token # for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=new_tokenizer, mlm_probability=0.15, return_tensors='pt')\n",
    "# Training setting\n",
    "# Freezing Larger part of DeBERTa for memory\n",
    "if model_repository == 'deberta':\n",
    "    for param in model.deberta.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "aim_callback = AimCallback(experiment=\"xlm-roberta-large model pretraining 512 tokens\")\n",
    "training_args = TrainingArguments( # Training argument 정리\n",
    "    output_dir=HF_CACHE_PATH + '/xlm_roberta_gbsqa_classification/large_model/pretraining',\n",
    "    evaluation_strategy= 'no',\n",
    "    save_strategy='no',\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=1000,\n",
    "    per_device_train_batch_size=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[aim_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(os.path.join(HF_CACHE_PATH, 'xlm_roberta_gbsqa_classification/revised_dataset_large_model/pretrained_tv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding text from pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoded text for classifier generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model loading\n",
    "pretrained_bert = AutoModel.from_pretrained(os.path.join(HF_CACHE_PATH, \"xlm_roberta_gbsqa_classification/revised_dataset_large_model/pretrained_tv\"))\n",
    "pretrained_bert.to('cuda')\n",
    "pretrained_bert.eval()\n",
    "def text_encode(total_input, total_labels):\n",
    "    encoding_list = []\n",
    "    for idx in range(len(total_labels)):\n",
    "        input_ = {'input_ids':None, 'attention_mask':None}\n",
    "        input_['input_ids'] = total_input['input_ids'][idx].view(1, -1).to('cuda')\n",
    "        input_['attention_mask'] = total_input['attention_mask'][idx].view(1, -1).to('cuda')\n",
    "        encoding_list.append(pretrained_bert(**input_).last_hidden_state.to('cpu').detach())\n",
    "        input_['input_ids'].detach()\n",
    "        input_['attention_mask'].detach()\n",
    "    return {'encode_list':encoding_list, 'labels':total_labels}\n",
    "\n",
    "# generate encoding\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    print(f'encoding {split}')\n",
    "    total_input = {'input_ids':[], 'attention_mask':[]}\n",
    "    total_labels = []\n",
    "    for item in tokenized_classification[split]:\n",
    "        total_input['input_ids'].append(item['input_ids'])\n",
    "        total_input['attention_mask'].append(item['attention_mask'])\n",
    "        total_labels.append(item['labels'])\n",
    "\n",
    "    total_input['input_ids'] = torch.stack(total_input['input_ids'], dim=0)\n",
    "    total_input['attention_mask'] = torch.stack(total_input['attention_mask'], dim=0)\n",
    "    total_labels = torch.stack(total_labels, dim=0)\n",
    "    to_save_pkl = text_encode(total_input, total_labels)\n",
    "    pickle.dump(to_save_pkl, open(f'./dataset/GBSQA_encode/{split}/xlm-roberta-large-revised-{split}-encode3.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NL_classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NL_classifier, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512 * 1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 2) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "class Encode_dataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "aim_session = Run(experiment='simple_2_layer_classifier')\n",
    "\n",
    "classifier = NL_classifier()\n",
    "epoch = 200\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=learning_rate)\n",
    "target_splits = ['train']\n",
    "class_x = None\n",
    "class_y = None\n",
    "\n",
    "\n",
    "for split in target_splits:\n",
    "    test_encode_load = pickle.load(open(f'./dataset/GBSQA_encode/{split}/xlm-roberta-large-revised-{split}-encode3.pkl', \"rb\"))\n",
    "    if (class_x is None) and (class_y is None):\n",
    "        class_x, class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "    else:\n",
    "        _class_x, _class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "        class_x = torch.stack([*class_x, *_class_x], dim=0)\n",
    "        class_y = torch.stack([*class_y, *_class_y], dim=0)\n",
    "if isinstance(class_x, list):\n",
    "    class_x = torch.stack([*class_x], dim=0)\n",
    "    class_y = torch.stack([*class_y], dim=0)\n",
    "class_x = class_x.view(class_x.shape[0], -1)\n",
    "objective_fn = torch.nn.CrossEntropyLoss()\n",
    "classifier = classifier.to('cuda')\n",
    "dset = Encode_dataset(class_x, class_y)\n",
    "dloader = DataLoader(dset, batch_size=16, shuffle=True)\n",
    "steps = 0\n",
    "for ep in range(epoch):\n",
    "    for x, y in dloader:\n",
    "        steps += 1\n",
    "        x = x.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        logits = classifier(x)\n",
    "        loss = objective_fn(logits, y)\n",
    "        aim_session.track(loss.item(), 'train_loss', steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "torch.save(classifier, './gbsqa_revised_classifier3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NL_classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NL_classifier, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512 * 1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 2) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "classifier = torch.load('gbsqa_revised_classifier3.pt')\n",
    "target_splits = ['test']\n",
    "class_x = None\n",
    "class_y = None\n",
    "\n",
    "for split in target_splits:\n",
    "    test_encode_load = pickle.load(open(f'./dataset/GBSQA_encode/{split}/xlm-roberta-large-revised-{split}-encode3.pkl', \"rb\"))\n",
    "    if (class_x is None) and (class_y is None):\n",
    "        class_x, class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "    else:\n",
    "        _class_x, _class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "        class_x = torch.stack([*class_x, *_class_x], dim=0)\n",
    "        class_y = torch.stack([*class_y, *_class_y], dim=0)\n",
    "if isinstance(class_x, list):\n",
    "    class_x = torch.stack([*class_x], dim=0)\n",
    "    class_y = torch.stack([*class_y], dim=0)\n",
    "\n",
    "class_x = class_x.view(class_x.shape[0], -1)\n",
    "classifier = classifier.to('cpu')\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "out = classifier(class_x)\n",
    "\n",
    "prediction = np.argmax(out.detach().numpy(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': {'accuracy': 0.85}, 'recall': {'recall': 1.0}, 'precision': {'precision': 0.8333333333333334}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_metric = evaluate.load('accuracy')\n",
    "recall_metric = evaluate.load('recall')\n",
    "precision_metric = evaluate.load('precision')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    recall = recall_metric.compute(predictions=predictions.tolist(), references=labels.tolist())\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    return {'accuracy': accuracy, 'recall':recall, 'precision':precision}\n",
    "\n",
    "print(compute_metrics((out.detach(), class_y.detach())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SentenceTransformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
