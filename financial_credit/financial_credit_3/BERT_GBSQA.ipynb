{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangun/anaconda3/envs/aidep/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sangun/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# path 변수.\n",
    "GBSQA_PATH = './dataset/GBSQA/GBS_QA_FF.csv' \n",
    "GBSQA_HOME_PATH = './dataset/GBSQA'\n",
    "HF_CACHE_PATH = './HuggingFaceCache/models' # huggingface에서 받는 모델이 저장되는 곳\n",
    "DATA_CACHE_PATH = '/HuggingFaceCache/dataset' # huggingface에서 받는 데이터셋이 저장되는 곳\n",
    "os.environ['TRANSFORMERS_CACHE'] = HF_CACHE_PATH # huggingface 캐쉬 디렉토리 지정. 모델이 다운 받는 위치 지정.\n",
    "os.environ['HF_DATASETS_CACHE'] = DATA_CACHE_PATH # 데이터셋 다운 받는 위치 지정\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' # PCI BUS ID로 현재 보이는 그래픽카드 정렬\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2' # 내가 할당된 그래픽카드만 시스템에서 보이도록 설정.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, AutoModel\n",
    "torch.cuda.current_device() # 0으로 출력되지만, 실제로 모델을 넣어 구동하면 지정한 그래픽카드에 할당될 것임.\n",
    "import pickle\n",
    "import evaluate\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from aim.hugging_face import AimCallback\n",
    "from aim import Run\n",
    "\n",
    "# For reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(li): # dataset 구분\n",
    "    num_test = 20 # 최소한의 test set 갯수 확보\n",
    "    train = li[:int(len(li) * 0.7)]\n",
    "    valid = li[int(len(li) * 0.7): len(li) - num_test]\n",
    "    test = li[len(li) - num_test:]\n",
    "    return train, valid, test\n",
    "\n",
    "def save_shuffle_indices(shuffled_pair, original_pair):\n",
    "    _o_pair = np.array(original_pair)\n",
    "    _s_pair = np.array(shuffled_pair)\n",
    "    find_idx_func = lambda x: np.where((_o_pair[:, 0] == x[0]) & (_o_pair[:, 1] == x[1]))\n",
    "    return np.array(list(map(find_idx_func, _s_pair))).squeeze()\n",
    "    \n",
    "def shuffle_array(array):\n",
    "    np.random.shuffle(array)\n",
    "    return array\n",
    "\n",
    "##\n",
    "# get ready dataset\n",
    "def load_preprocess_data(fpath: os.PathLike = './dataset/GBSQA/provision_intersected.csv'): \n",
    "\n",
    "    gbsqa = pd.read_csv(GBSQA_HOME_PATH + '/provision_intersected.csv')\n",
    "    valid_gbsqa = gbsqa[~gbsqa['provision'].isna()] # 전문가의 자세한 답변이 담긴 사항이 없는 것들 제외.\n",
    "    valid_gbsqa = valid_gbsqa[~valid_gbsqa['Classification'].isna()] # classification이 있는 것만 추출.\n",
    "    valid_gbsqa.drop_duplicates(subset=['Original answers', 'provision_text'], inplace=True)\n",
    "\n",
    "    text_pair = valid_gbsqa[['Original answers', 'provision_text']]\n",
    "    text_pair = list(text_pair.itertuples(index=False, name=None))\n",
    "    shuffled_pair = shuffle_array(text_pair.copy())\n",
    "    shuffle_indices = save_shuffle_indices(shuffled_pair, text_pair)\n",
    "\n",
    "    text_pair = valid_gbsqa[['Original answers', 'provision_text']]\n",
    "    train_indices, valid_indices, test_indices = dataset_split(shuffle_indices)\n",
    "    raw_dataset = {'train': text_pair.iloc[train_indices], 'valid': text_pair.iloc[valid_indices], 'test': text_pair.iloc[test_indices]}\n",
    "    \n",
    "    return raw_dataset, valid_gbsqa, shuffle_indices\n",
    "\n",
    "raw_dataset, valid_gbsqa, shuffle_indices = load_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_tokenizer = AutoTokenizer.from_pretrained(HF_CACHE_PATH + '/xlm_roberta_gbsqa_classification/tokenizer')\n",
    "\n",
    "labels = valid_gbsqa['Classification']\n",
    "zero_one = labels.map({'Yes':1, 'No':0})\n",
    "\n",
    "valid_gbsqa = valid_gbsqa[~zero_one.isna()]\n",
    "valid_gbsqa['Classification'] = zero_one\n",
    "\n",
    "def shuffle_array(array):\n",
    "    np.random.shuffle(array)\n",
    "    return array\n",
    "\n",
    "def tokenize_preprocess_classification(examples):\n",
    "    return class_tokenizer(*examples,max_length=512, padding='max_length', truncation=True)\n",
    "\n",
    "def dataset_split(li):\n",
    "    num_test = 20\n",
    "    train = li[:int(len(li) * 0.7)]\n",
    "    valid = li[int(len(li) * 0.7): len(li) - num_test]\n",
    "    test = li[len(li) - num_test:]\n",
    "    return train, valid, test\n",
    "\n",
    "def save_shuffle_indices(shuffled_pair, original_pair):\n",
    "    _o_pair = np.array(original_pair)\n",
    "    _s_pair = np.array(shuffled_pair)\n",
    "    find_idx_func = lambda x: np.where((_o_pair[:, 0] == x[0]) & (_o_pair[:, 1] == x[1]))\n",
    "    return np.array(list(map(find_idx_func, _s_pair))).squeeze()\n",
    "\n",
    "\n",
    "text_pair = valid_gbsqa[['Original answers', 'Revised questions', 'Classification']] \n",
    "text_pair = text_pair.iloc[shuffle_indices]\n",
    "text_pair = list(text_pair.itertuples(index=False, name=None))\n",
    "tokenized_classification = {}\n",
    "tp_np = np.array(text_pair)\n",
    "\n",
    "raw_dataset_classification_train, raw_dataset_classification_valid, raw_dataset_classification_test = dataset_split(tp_np.tolist())\n",
    "\n",
    "class GBSQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        self.length = len(encodings)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "tokenized_classification['train'] = GBSQADataset([dict({'labels': int(float(label))}, **tokenize_preprocess_classification([text_a, text_b])) for text_a, text_b, label in raw_dataset_classification_train])\n",
    "tokenized_classification['valid'] = GBSQADataset([dict({'labels': int(float(label))}, **tokenize_preprocess_classification([text_a, text_b]))for text_a, text_b, label in raw_dataset_classification_valid])\n",
    "tokenized_classification['test'] = GBSQADataset([dict({'labels': int(float(label))}, **tokenize_preprocess_classification([text_a, text_b])) for text_a, text_b, label in raw_dataset_classification_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pretraining on revised datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Importing 셀 블록을 실행한 이후에 사용 가능.\n",
    "model_repository = {'roberta':'roberta-base', \"deberta\":\"microsoft/deberta-v3-base\", \"xlm-roberta\":\"xlm-roberta-base\", \"xlm-large\":\"xlm-roberta-large\"}\n",
    "model_selection = \"xlm-large\"\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(HF_CACHE_PATH + '/xlm_roberta_gbsqa_classification/tokenizer')\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_repository[model_selection]) # get ready from pre-trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_preprocess(examples):\n",
    "    return new_tokenizer(*examples, max_length=512,truncation=True) # 위에 정의한 new_tokenizer.\n",
    "train = list(raw_dataset['train'].to_numpy())\n",
    "valid = list(raw_dataset['valid'].to_numpy())\n",
    "tokenized_train = list(map(tokenize_preprocess,train))\n",
    "tokenized_valid = list(map(tokenize_preprocess, valid))\n",
    "tokenized_train.extend(tokenized_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangun/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m         param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m aim_callback \u001b[38;5;241m=\u001b[39m AimCallback(experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-large model pretraining 512 tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Training argument 정리\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHF_CACHE_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/xlm_roberta_gbsqa_classification/large_model/pretraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[aim_callback]\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m<string>:131\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, eval_use_gather_object)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/training_args.py:1730\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1733\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1737\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/training_args.py:2227\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2226\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/utils/generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/training_args.py:2103\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2104\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2106\u001b[0m         )\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2108\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "\n",
    "new_tokenizer.pad_token = new_tokenizer.eos_token # for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=new_tokenizer, mlm_probability=0.15, return_tensors='pt')\n",
    "# Training setting\n",
    "# Freezing Larger part of DeBERTa for memory\n",
    "if model_repository == 'deberta':\n",
    "    for param in model.deberta.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "aim_callback = AimCallback(experiment=\"xlm-roberta-large model pretraining 512 tokens\")\n",
    "training_args = TrainingArguments( # Training argument 정리\n",
    "    output_dir=HF_CACHE_PATH + '/xlm_roberta_gbsqa_classification/large_model/pretraining',\n",
    "    evaluation_strategy= 'no',\n",
    "    save_strategy='no',\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=1000,\n",
    "    per_device_train_batch_size=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[aim_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(os.path.join(HF_CACHE_PATH, 'xlm_roberta_gbsqa_classification/revised_dataset_large_model/pretrained_tv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding text from pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoded text for classifier generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model loading\n",
    "pretrained_bert = AutoModel.from_pretrained(os.path.join(HF_CACHE_PATH, \"xlm_roberta_gbsqa_classification/revised_dataset_large_model/pretrained_tv\"))\n",
    "pretrained_bert.to('cuda')\n",
    "pretrained_bert.eval()\n",
    "def text_encode(total_input, total_labels):\n",
    "    encoding_list = []\n",
    "    for idx in range(len(total_labels)):\n",
    "        input_ = {'input_ids':None, 'attention_mask':None}\n",
    "        input_['input_ids'] = total_input['input_ids'][idx].view(1, -1).to('cuda')\n",
    "        input_['attention_mask'] = total_input['attention_mask'][idx].view(1, -1).to('cuda')\n",
    "        encoding_list.append(pretrained_bert(**input_).last_hidden_state.to('cpu').detach())\n",
    "        input_['input_ids'].detach()\n",
    "        input_['attention_mask'].detach()\n",
    "    return {'encode_list':encoding_list, 'labels':total_labels}\n",
    "\n",
    "# generate encoding\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    print(f'encoding {split}')\n",
    "    total_input = {'input_ids':[], 'attention_mask':[]}\n",
    "    total_labels = []\n",
    "    for item in tokenized_classification[split]:\n",
    "        total_input['input_ids'].append(item['input_ids'])\n",
    "        total_input['attention_mask'].append(item['attention_mask'])\n",
    "        total_labels.append(item['labels'])\n",
    "\n",
    "    total_input['input_ids'] = torch.stack(total_input['input_ids'], dim=0)\n",
    "    total_input['attention_mask'] = torch.stack(total_input['attention_mask'], dim=0)\n",
    "    total_labels = torch.stack(total_labels, dim=0)\n",
    "    to_save_pkl = text_encode(total_input, total_labels)\n",
    "    pickle.dump(to_save_pkl, open(f'./dataset/GBSQA_encode/{split}/xlm-roberta-large-revised-{split}-encode3.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NL_classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NL_classifier, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512 * 1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 2) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "class Encode_dataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "aim_session = Run(experiment='simple_2_layer_classifier')\n",
    "\n",
    "classifier = NL_classifier()\n",
    "epoch = 200\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=learning_rate)\n",
    "target_splits = ['train']\n",
    "class_x = None\n",
    "class_y = None\n",
    "\n",
    "\n",
    "for split in target_splits:\n",
    "    test_encode_load = pickle.load(open(f'./dataset/GBSQA_encode/{split}/xlm-roberta-large-revised-{split}-encode3.pkl', \"rb\"))\n",
    "    if (class_x is None) and (class_y is None):\n",
    "        class_x, class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "    else:\n",
    "        _class_x, _class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "        class_x = torch.stack([*class_x, *_class_x], dim=0)\n",
    "        class_y = torch.stack([*class_y, *_class_y], dim=0)\n",
    "if isinstance(class_x, list):\n",
    "    class_x = torch.stack([*class_x], dim=0)\n",
    "    class_y = torch.stack([*class_y], dim=0)\n",
    "class_x = class_x.view(class_x.shape[0], -1)\n",
    "objective_fn = torch.nn.CrossEntropyLoss()\n",
    "classifier = classifier.to('cuda')\n",
    "dset = Encode_dataset(class_x, class_y)\n",
    "dloader = DataLoader(dset, batch_size=16, shuffle=True)\n",
    "steps = 0\n",
    "for ep in range(epoch):\n",
    "    for x, y in dloader:\n",
    "        steps += 1\n",
    "        x = x.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        logits = classifier(x)\n",
    "        loss = objective_fn(logits, y)\n",
    "        aim_session.track(loss.item(), 'train_loss', steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "torch.save(classifier, './gbsqa_revised_classifier3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NL_classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NL_classifier, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512 * 1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 2) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "classifier = torch.load('gbsqa_revised_classifier3.pt')\n",
    "target_splits = ['test']\n",
    "class_x = None\n",
    "class_y = None\n",
    "\n",
    "for split in target_splits:\n",
    "    test_encode_load = pickle.load(open(f'./dataset/GBSQA_encode/{split}/xlm-roberta-large-revised-{split}-encode3.pkl', \"rb\"))\n",
    "    if (class_x is None) and (class_y is None):\n",
    "        class_x, class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "    else:\n",
    "        _class_x, _class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "        class_x = torch.stack([*class_x, *_class_x], dim=0)\n",
    "        class_y = torch.stack([*class_y, *_class_y], dim=0)\n",
    "if isinstance(class_x, list):\n",
    "    class_x = torch.stack([*class_x], dim=0)\n",
    "    class_y = torch.stack([*class_y], dim=0)\n",
    "\n",
    "class_x = class_x.view(class_x.shape[0], -1)\n",
    "classifier = classifier.to('cpu')\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "out = classifier(class_x)\n",
    "\n",
    "prediction = np.argmax(out.detach().numpy(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': {'accuracy': 0.85}, 'recall': {'recall': 1.0}, 'precision': {'precision': 0.8333333333333334}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_metric = evaluate.load('accuracy')\n",
    "recall_metric = evaluate.load('recall')\n",
    "precision_metric = evaluate.load('precision')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    recall = recall_metric.compute(predictions=predictions.tolist(), references=labels.tolist())\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    return {'accuracy': accuracy, 'recall':recall, 'precision':precision}\n",
    "\n",
    "print(compute_metrics((out.detach(), class_y.detach())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SentenceTransformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
