{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangun/anaconda3/envs/aidep/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sangun/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# path ë³€ìˆ˜.\n",
    "GBSQA_PATH = './dataset/GBSQA/GBS_QA_FF.csv' \n",
    "GBSQA_HOME_PATH = './dataset/GBSQA'\n",
    "HF_CACHE_PATH = './HuggingFaceCache/models' # huggingfaceì—ì„œ ë°›ëŠ” ëª¨ë¸ì´ ì €ìž¥ë˜ëŠ” ê³³\n",
    "DATA_CACHE_PATH = '/HuggingFaceCache/dataset' # huggingfaceì—ì„œ ë°›ëŠ” ë°ì´í„°ì…‹ì´ ì €ìž¥ë˜ëŠ” ê³³\n",
    "os.environ['TRANSFORMERS_CACHE'] = HF_CACHE_PATH # huggingface ìºì‰¬ ë””ë ‰í† ë¦¬ ì§€ì •. ëª¨ë¸ì´ ë‹¤ìš´ ë°›ëŠ” ìœ„ì¹˜ ì§€ì •.\n",
    "os.environ['HF_DATASETS_CACHE'] = DATA_CACHE_PATH # ë°ì´í„°ì…‹ ë‹¤ìš´ ë°›ëŠ” ìœ„ì¹˜ ì§€ì •\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' # PCI BUS IDë¡œ í˜„ìž¬ ë³´ì´ëŠ” ê·¸ëž˜í”½ì¹´ë“œ ì •ë ¬\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2' # ë‚´ê°€ í• ë‹¹ëœ ê·¸ëž˜í”½ì¹´ë“œë§Œ ì‹œìŠ¤í…œì—ì„œ ë³´ì´ë„ë¡ ì„¤ì •.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, AutoModel\n",
    "torch.cuda.current_device() # 0ìœ¼ë¡œ ì¶œë ¥ë˜ì§€ë§Œ, ì‹¤ì œë¡œ ëª¨ë¸ì„ ë„£ì–´ êµ¬ë™í•˜ë©´ ì§€ì •í•œ ê·¸ëž˜í”½ì¹´ë“œì— í• ë‹¹ë  ê²ƒìž„.\n",
    "import pickle\n",
    "import evaluate\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from aim.hugging_face import AimCallback\n",
    "from aim import Run\n",
    "\n",
    "# For reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining ë°ì´í„° ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(li): # dataset êµ¬ë¶„\n",
    "    num_test = 20 # ìµœì†Œí•œì˜ test set ê°¯ìˆ˜ í™•ë³´\n",
    "    train = li[:int(len(li) * 0.7)]\n",
    "    valid = li[int(len(li) * 0.7): len(li) - num_test]\n",
    "    test = li[len(li) - num_test:]\n",
    "    return train, valid, test\n",
    "\n",
    "def save_shuffle_indices(shuffled_pair, original_pair):\n",
    "    _o_pair = np.array(original_pair)\n",
    "    _s_pair = np.array(shuffled_pair)\n",
    "    find_idx_func = lambda x: np.where((_o_pair[:, 0] == x[0]) & (_o_pair[:, 1] == x[1]))\n",
    "    return np.array(list(map(find_idx_func, _s_pair))).squeeze()\n",
    "    \n",
    "def shuffle_array(array):\n",
    "    np.random.shuffle(array)\n",
    "    return array\n",
    "\n",
    "##\n",
    "# get ready dataset\n",
    "def load_preprocess_data(fpath: os.PathLike = './dataset/GBSQA/provision_intersected.csv'): \n",
    "\n",
    "    gbsqa = pd.read_csv(GBSQA_HOME_PATH + '/provision_intersected.csv')\n",
    "    valid_gbsqa = gbsqa[~gbsqa['provision'].isna()] # ì „ë¬¸ê°€ì˜ ìžì„¸í•œ ë‹µë³€ì´ ë‹´ê¸´ ì‚¬í•­ì´ ì—†ëŠ” ê²ƒë“¤ ì œì™¸.\n",
    "    valid_gbsqa = valid_gbsqa[~valid_gbsqa['Classification'].isna()] # classificationì´ ìžˆëŠ” ê²ƒë§Œ ì¶”ì¶œ.\n",
    "    valid_gbsqa.drop_duplicates(subset=['Original answers', 'provision_text'], inplace=True)\n",
    "\n",
    "    text_pair = valid_gbsqa[['Original answers', 'provision_text']]\n",
    "    text_pair = list(text_pair.itertuples(index=False, name=None))\n",
    "    shuffled_pair = shuffle_array(text_pair.copy())\n",
    "    shuffle_indices = save_shuffle_indices(shuffled_pair, text_pair)\n",
    "\n",
    "    text_pair = valid_gbsqa[['Original answers', 'provision_text']]\n",
    "    train_indices, valid_indices, test_indices = dataset_split(shuffle_indices)\n",
    "    raw_dataset = {'train': text_pair.iloc[train_indices], 'valid': text_pair.iloc[valid_indices], 'test': text_pair.iloc[test_indices]}\n",
    "    \n",
    "    return raw_dataset, valid_gbsqa, shuffle_indices\n",
    "\n",
    "raw_dataset, valid_gbsqa, shuffle_indices = load_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification dataset ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_tokenizer = AutoTokenizer.from_pretrained(HF_CACHE_PATH + '/xlm_roberta_gbsqa_classification/tokenizer')\n",
    "\n",
    "labels = valid_gbsqa['Classification']\n",
    "zero_one = labels.map({'Yes':1, 'No':0})\n",
    "\n",
    "valid_gbsqa = valid_gbsqa[~zero_one.isna()]\n",
    "valid_gbsqa['Classification'] = zero_one\n",
    "\n",
    "def shuffle_array(array):\n",
    "    np.random.shuffle(array)\n",
    "    return array\n",
    "\n",
    "def tokenize_preprocess_classification(examples):\n",
    "    return class_tokenizer(*examples,max_length=512, padding='max_length', truncation=True)\n",
    "\n",
    "def dataset_split(li):\n",
    "    num_test = 20\n",
    "    train = li[:int(len(li) * 0.7)]\n",
    "    valid = li[int(len(li) * 0.7): len(li) - num_test]\n",
    "    test = li[len(li) - num_test:]\n",
    "    return train, valid, test\n",
    "\n",
    "def save_shuffle_indices(shuffled_pair, original_pair):\n",
    "    _o_pair = np.array(original_pair)\n",
    "    _s_pair = np.array(shuffled_pair)\n",
    "    find_idx_func = lambda x: np.where((_o_pair[:, 0] == x[0]) & (_o_pair[:, 1] == x[1]))\n",
    "    return np.array(list(map(find_idx_func, _s_pair))).squeeze()\n",
    "\n",
    "\n",
    "text_pair = valid_gbsqa[['Original answers', 'Revised questions', 'Classification']] \n",
    "text_pair = text_pair.iloc[shuffle_indices]\n",
    "text_pair = list(text_pair.itertuples(index=False, name=None))\n",
    "tokenized_classification = {}\n",
    "tp_np = np.array(text_pair)\n",
    "\n",
    "raw_dataset_classification_train, raw_dataset_classification_valid, raw_dataset_classification_test = dataset_split(tp_np.tolist())\n",
    "\n",
    "class GBSQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        self.length = len(encodings)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "tokenized_classification['train'] = GBSQADataset([dict({'labels': int(float(label))}, **tokenize_preprocess_classification([text_a, text_b])) for text_a, text_b, label in raw_dataset_classification_train])\n",
    "tokenized_classification['valid'] = GBSQADataset([dict({'labels': int(float(label))}, **tokenize_preprocess_classification([text_a, text_b]))for text_a, text_b, label in raw_dataset_classification_valid])\n",
    "tokenized_classification['test'] = GBSQADataset([dict({'labels': int(float(label))}, **tokenize_preprocess_classification([text_a, text_b])) for text_a, text_b, label in raw_dataset_classification_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pretraining on revised datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Importing ì…€ ë¸”ë¡ì„ ì‹¤í–‰í•œ ì´í›„ì— ì‚¬ìš© ê°€ëŠ¥.\n",
    "model_repository = {'roberta':'roberta-base', \"deberta\":\"microsoft/deberta-v3-base\", \"xlm-roberta\":\"xlm-roberta-base\", \"xlm-large\":\"xlm-roberta-large\"}\n",
    "model_selection = \"xlm-large\"\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(HF_CACHE_PATH + '/xlm_roberta_gbsqa_classification/tokenizer')\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_repository[model_selection]) # get ready from pre-trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_preprocess(examples):\n",
    "    return new_tokenizer(*examples, max_length=512,truncation=True) # ìœ„ì— ì •ì˜í•œ new_tokenizer.\n",
    "train = list(raw_dataset['train'].to_numpy())\n",
    "valid = list(raw_dataset['valid'].to_numpy())\n",
    "tokenized_train = list(map(tokenize_preprocess,train))\n",
    "tokenized_valid = list(map(tokenize_preprocess, valid))\n",
    "tokenized_train.extend(tokenized_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangun/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m         param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m aim_callback \u001b[38;5;241m=\u001b[39m AimCallback(experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-large model pretraining 512 tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Training argument ì •ë¦¬\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHF_CACHE_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/xlm_roberta_gbsqa_classification/large_model/pretraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[aim_callback]\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m<string>:131\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, eval_use_gather_object)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/training_args.py:1730\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1733\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1737\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/training_args.py:2227\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2226\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/utils/generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/anaconda3/envs/aidep/lib/python3.9/site-packages/transformers/training_args.py:2103\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2104\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2106\u001b[0m         )\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2108\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "\n",
    "new_tokenizer.pad_token = new_tokenizer.eos_token # for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=new_tokenizer, mlm_probability=0.15, return_tensors='pt')\n",
    "# Training setting\n",
    "# Freezing Larger part of DeBERTa for memory\n",
    "if model_repository == 'deberta':\n",
    "    for param in model.deberta.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "aim_callback = AimCallback(experiment=\"xlm-roberta-large model pretraining 512 tokens\")\n",
    "training_args = TrainingArguments( # Training argument ì •ë¦¬\n",
    "    output_dir=HF_CACHE_PATH + '/xlm_roberta_gbsqa_classification/large_model/pretraining',\n",
    "    evaluation_strategy= 'no',\n",
    "    save_strategy='no',\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=1000,\n",
    "    per_device_train_batch_size=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[aim_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(os.path.join(HF_CACHE_PATH, 'xlm_roberta_gbsqa_classification/revised_dataset_large_model/pretrained_tv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding text from pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoded text for classifier generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model loading\n",
    "pretrained_bert = AutoModel.from_pretrained(os.path.join(HF_CACHE_PATH, \"xlm_roberta_gbsqa_classification/revised_dataset_large_model/pretrained_tv\"))\n",
    "pretrained_bert.to('cuda')\n",
    "pretrained_bert.eval()\n",
    "def text_encode(total_input, total_labels):\n",
    "    encoding_list = []\n",
    "    for idx in range(len(total_labels)):\n",
    "        input_ = {'input_ids':None, 'attention_mask':None}\n",
    "        input_['input_ids'] = total_input['input_ids'][idx].view(1, -1).to('cuda')\n",
    "        input_['attention_mask'] = total_input['attention_mask'][idx].view(1, -1).to('cuda')\n",
    "        encoding_list.append(pretrained_bert(**input_).last_hidden_state.to('cpu').detach())\n",
    "        input_['input_ids'].detach()\n",
    "        input_['attention_mask'].detach()\n",
    "    return {'encode_list':encoding_list, 'labels':total_labels}\n",
    "\n",
    "# generate encoding\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    print(f'encoding {split}')\n",
    "    total_input = {'input_ids':[], 'attention_mask':[]}\n",
    "    total_labels = []\n",
    "    for item in tokenized_classification[split]:\n",
    "        total_input['input_ids'].append(item['input_ids'])\n",
    "        total_input['attention_mask'].append(item['attention_mask'])\n",
    "        total_labels.append(item['labels'])\n",
    "\n",
    "    total_input['input_ids'] = torch.stack(total_input['input_ids'], dim=0)\n",
    "    total_input['attention_mask'] = torch.stack(total_input['attention_mask'], dim=0)\n",
    "    total_labels = torch.stack(total_labels, dim=0)\n",
    "    to_save_pkl = text_encode(total_input, total_labels)\n",
    "    pickle.dump(to_save_pkl, open(f'./dataset/GBSQA_encode/{split}/xlm-roberta-large-revised-{split}-encode3.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NL_classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NL_classifier, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512 * 1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 2) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "class Encode_dataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "aim_session = Run(experiment='simple_2_layer_classifier')\n",
    "\n",
    "classifier = NL_classifier()\n",
    "epoch = 200\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=learning_rate)\n",
    "target_splits = ['train']\n",
    "class_x = None\n",
    "class_y = None\n",
    "\n",
    "\n",
    "for split in target_splits:\n",
    "    test_encode_load = pickle.load(open(f'./dataset/GBSQA_encode/{split}/xlm-roberta-large-revised-{split}-encode3.pkl', \"rb\"))\n",
    "    if (class_x is None) and (class_y is None):\n",
    "        class_x, class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "    else:\n",
    "        _class_x, _class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "        class_x = torch.stack([*class_x, *_class_x], dim=0)\n",
    "        class_y = torch.stack([*class_y, *_class_y], dim=0)\n",
    "if isinstance(class_x, list):\n",
    "    class_x = torch.stack([*class_x], dim=0)\n",
    "    class_y = torch.stack([*class_y], dim=0)\n",
    "class_x = class_x.view(class_x.shape[0], -1)\n",
    "objective_fn = torch.nn.CrossEntropyLoss()\n",
    "classifier = classifier.to('cuda')\n",
    "dset = Encode_dataset(class_x, class_y)\n",
    "dloader = DataLoader(dset, batch_size=16, shuffle=True)\n",
    "steps = 0\n",
    "for ep in range(epoch):\n",
    "    for x, y in dloader:\n",
    "        steps += 1\n",
    "        x = x.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        logits = classifier(x)\n",
    "        loss = objective_fn(logits, y)\n",
    "        aim_session.track(loss.item(), 'train_loss', steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "torch.save(classifier, './gbsqa_revised_classifier3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NL_classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NL_classifier, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512 * 1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 2) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "classifier = torch.load('gbsqa_revised_classifier3.pt')\n",
    "target_splits = ['test']\n",
    "class_x = None\n",
    "class_y = None\n",
    "\n",
    "for split in target_splits:\n",
    "    test_encode_load = pickle.load(open(f'./dataset/GBSQA_encode/{split}/xlm-roberta-large-revised-{split}-encode3.pkl', \"rb\"))\n",
    "    if (class_x is None) and (class_y is None):\n",
    "        class_x, class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "    else:\n",
    "        _class_x, _class_y = test_encode_load['encode_list'], test_encode_load['labels']\n",
    "        class_x = torch.stack([*class_x, *_class_x], dim=0)\n",
    "        class_y = torch.stack([*class_y, *_class_y], dim=0)\n",
    "if isinstance(class_x, list):\n",
    "    class_x = torch.stack([*class_x], dim=0)\n",
    "    class_y = torch.stack([*class_y], dim=0)\n",
    "\n",
    "class_x = class_x.view(class_x.shape[0], -1)\n",
    "classifier = classifier.to('cpu')\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "out = classifier(class_x)\n",
    "\n",
    "prediction = np.argmax(out.detach().numpy(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': {'accuracy': 0.85}, 'recall': {'recall': 1.0}, 'precision': {'precision': 0.8333333333333334}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_metric = evaluate.load('accuracy')\n",
    "recall_metric = evaluate.load('recall')\n",
    "precision_metric = evaluate.load('precision')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    recall = recall_metric.compute(predictions=predictions.tolist(), references=labels.tolist())\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    return {'accuracy': accuracy, 'recall':recall, 'precision':precision}\n",
    "\n",
    "print(compute_metrics((out.detach(), class_y.detach())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SentenceTransformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
