# Prompt Tuning for Enhanced Non-Native Speech Recognition 

<!-- ABOUT THE PROJECT -->
## About The Project

This project focuses on improving Automatic Speech Recognition (ASR) systems for non-native/accented English utterance without modifying the underlying pre-trained model. INTapt introduces a novel approach using adversarially trained prompts that are concatenated to the input audio. These prompts guide the ASR model to better process non-native accents by aligning their features closer to native English accents.

<!-- Usage -->
## Usage

This is an example of how you may give instructions on setting up your project locally.
To get a local copy up and running follow these simple example steps.

### 0. Dependencies

create a conda environment from `enviromentve.yaml` file

  ```sh
  conda env create --name ENV_NAME --file INTapt/environment.yaml
  conda activate ENV_NAME
  ```

### 1. Run experiments
```sh
python INTapt/ASR_INTapt.py --do_model_download --eval_mode intapt --eval_metric wer
```
#### Initial Setup

- **Downloading Models**: When running the code for the first time, you must use the `--do_model_download` option to download the HuBERT and Prompt Generator models. After the initial download, this option is no longer required for subsequent runs.
- **Evaluation Modes**: You can select the evaluation mode using the `--eval_mode` option. The available modes are:
  - `intapt`: Uses prompts for evaluation.
  - `base`: Does not use prompts.
- **Evaluation Metrics**: Choose the evaluation metric with the `--eval_metric` option. Supported metrics are:
  - `wer` (Word Error Rate)
  - `cer` (Character Error Rate)

#### Dataset and Model Caching

- The current code is configured to use the **CORAAL** dataset.
- The `hf_cache_dir` option specifies the Hugging Face cache directory where models are downloaded. By default, it is set to `INTapt/hf_cache`.

#### Results

- The results include:
  - WER/CER calculated for each speaker.
  - Average WER/CER calculated across all speakers.

#### Models and Prompts

- **Base Model**: This mode does not use prompts for evaluation.
- **INTapt Model**: This mode uses prompts generated by the prompt generator model.
- The prompt generator model used is `INTapt-HuBERT-large-coraal-prompt-generator`.

#### Configuration Options

- **Batch Size**: The default batch size is set to 8. You can modify it using the `--batch_size` option.
- **Prompt Length**: The prompt length is set to 40 by default. It is recommended not to change this value, as the trained model is optimized for this length.



<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag "enhancement".
Don't forget to give the project a star! Thanks again!

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

<p align="right">(<a href="#readme-top">back to top</a>)</p>


## License

Distributed under the MIT License. See `LICENSE.txt` for more information.

<p align="right">(<a href="#readme-top">back to top</a>)</p>


<!-- CONTACT -->
## Contact

Eunseop Yoon - esyoon97@kaist.ac.kr


<p align="right">(<a href="#readme-top">back to top</a>)</p>
